<h1>Waarom gebruiken we Kinect 2.0?</h1>
<hr />

<p>Voor de interactie met de interactive mirror zijn we eerst mogelijke concepten gaan bedenken wat we allemaal met de mirror zouden kunnen doen. Deze staan beschreven in het mapje concepten. Hierna zijn we gaan kijken welke technieken we allemaal nodig hebben om deze concepten te realiseren. Hiermee kwamen we tot het volgende lijstje: </p>
<ul>
  <li>Voice commands.</li>
  <li>Gestures.</li>
  <li>Eye tracking.</li>
  <li>Face Recognition.</li>
  <li>Indentificatie met een Smartphone.</li>
</ul>

<p>Deze zijn we verder gaan uitzoeken in de vorm van mogelijke sensoren en tools waarmee we deze kunnen realiseren. Mogelijke sensoren/tools:</p>
<ul>
  <li>Tobii voor besturing door middel van eye tracking.</li>
  <li>Leap Motion voor besturing door middel van gestures</li>
  <li>Kinect V2 voor besturing door middel van gestures.</li>
  <li>Webcam voor face recognition.</li>
  <li>Microfoon voor besturing door middel van voice commands.</li>
  <li>Smartphone voor identificatie.</li>
</ul>

<p>Uiteindelijk hebben we als basis concept gekozen voor het “HUB” concept voor het realiseren van een prototype interactive mirror. Deze keuze is gemaakt omdat dit concept de meeste mogelijkheden bied met het oog op toekomstige uitbreidingen.
Voor de interactie hebben we voor Kinect v2 gekozen omdat we daarmee maar 1 sensor nodig hebben voor de integratie van:</p>
<ul>
  <li>Gestures</li>
  <li>Voice commands</li>
  <li>Face Recognition</li>
</ul>

<p>Het feit dat we alle technieken waarmee we wilden gaan werken door middel van een sensor konden gebruiken heeft de doorslag gegeven om voor Kinect v2 te kiezen en niet voor meerdere losse sensoren.</p>
